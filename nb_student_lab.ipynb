{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {"provenance": []},
    "kernelspec": {"name": "python3", "display_name": "Python 3"},
    "language_info": {"name": "python"}
  },
  "cells": [
    {"cell_type":"markdown","metadata":{},"source":[
      "# Naive Bayes — Student Lab\n",
      "\n",
      "Implement Multinomial Naive Bayes from scratch (log-space + Laplace smoothing)." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "import re\n",
      "import numpy as np\n",
      "\n",
      "def check(name: str, cond: bool):\n",
      "    if not cond:\n",
      "        raise AssertionError(f'Failed: {name}')\n",
      "    print(f'OK: {name}')\n",
      "\n",
      "rng = np.random.default_rng(0)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 0 — Toy dataset\n",
      "We use a tiny spam/ham dataset so you can verify logic easily." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "docs = [\n",
      "    ('spam', 'win money now'),\n",
      "    ('spam', 'free money win'),\n",
      "    ('spam', 'claim your free prize'),\n",
      "    ('ham',  'meeting schedule tomorrow'),\n",
      "    ('ham',  'let us schedule a meeting'),\n",
      "    ('ham',  'project update tomorrow'),\n",
      "]\n",
      "\n",
      "labels = np.array([1 if y=='spam' else 0 for y,_ in docs])\n",
      "texts = [t for _,t in docs]"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 1 — Tokenization + Vocabulary\n",
      "\n",
      "### Task 1.1: Tokenize\n",
      "\n",
      "# TODO: implement tokenizer\n",
      "# HINT: lowercase + split on non-letters\n",
      "\n",
      "**FAANG gotcha:** punctuation/case can break counts." 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def tokenize(text):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "print(tokenize('Free MONEY!!!'))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "### Task 1.2: Build vocabulary\n",
      "\n",
      "# TODO: build vocab dict word->index from all docs\n",
      "\n",
      "**Checkpoint:** What happens if a word appears only in test?" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "# TODO\n",
      "vocab = ...\n",
      "print(vocab)\n",
      "check('vocab_nonempty', len(vocab) > 0)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 2 — Vectorization (Count vectors)\n",
      "\n",
      "### Task 2.1: Convert docs to count matrix X (n_docs, |V|)\n",
      "\n",
      "# TODO\n",
      "# HINT: loop over tokens inside a doc is ok; avoid loops over vocab per doc\n" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def vectorize(texts, vocab):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "X = vectorize(texts, vocab)\n",
      "print('X shape', X.shape)\n",
      "check('shape', X.shape[0] == len(texts) and X.shape[1] == len(vocab))"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 3 — Train Multinomial Naive Bayes\n",
      "\n",
      "### Task 3.1: Fit with Laplace smoothing\n",
      "\n",
      "Compute:\n",
      "- class priors P(y)\n",
      "- word likelihoods P(word|y) with Laplace smoothing alpha\n",
      "\n",
      "# HINT:\n",
      "- count words per class: sum rows where y==c\n",
      "- add alpha to each word count\n",
      "\n",
      "**Checkpoint:** Why smoothing is required?" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def fit_nb(X, y, alpha=1.0):\n",
      "    # TODO\n",
      "    # return log_priors (2,), log_likelihoods (2, V)\n",
      "    ...\n",
      "\n",
      "log_priors, log_lik = fit_nb(X, labels, alpha=1.0)\n",
      "check('priors_shape', log_priors.shape == (2,))\n",
      "check('lik_shape', log_lik.shape[0] == 2 and log_lik.shape[1] == X.shape[1])"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "### Task 3.2: Predict in log-space\n",
      "\n",
      "For each doc:\n",
      "log P(y=c|x) ∝ log P(y=c) + sum_j x_j * log P(word_j|c)\n",
      "\n",
      "**Interview Angle:** Why use logs?" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "def predict_nb(X, log_priors, log_lik):\n",
      "    # TODO\n",
      "    ...\n",
      "\n",
      "pred = predict_nb(X, log_priors, log_lik)\n",
      "print('pred', pred)\n",
      "acc = float(np.mean(pred == labels))\n",
      "print('train_acc', acc)\n",
      "check('acc_reasonable', acc >= 0.8)"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "## Section 4 — Error Analysis\n",
      "\n",
      "### Task 4.1: Most influential tokens per class\n",
      "Show top tokens by log-likelihood ratio between classes.\n",
      "\n",
      "# HINT: sort (log_lik[spam] - log_lik[ham])\n" 
    ]},
    {"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[
      "# TODO\n",
      "inv_vocab = {i:w for w,i in vocab.items()}\n",
      "score = ...\n",
      "top = ...\n",
      "print('top spam tokens:', [inv_vocab[i] for i in top])"
    ]},

    {"cell_type":"markdown","metadata":{},"source":[
      "---\n",
      "## Submission Checklist\n",
      "- All TODOs completed\n",
      "- Train accuracy shown\n",
      "- Top tokens printed\n",
      "- Checkpoint questions answered\n"
    ]}
  ]
}
