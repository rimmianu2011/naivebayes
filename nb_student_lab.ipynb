{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "28HVyZx1Q9MG"
      },
      "source": [
        "# Naive Bayes — Student Lab\n",
        "\n",
        "Implement Multinomial Naive Bayes from scratch (log-space + Laplace smoothing)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "wsENTX22Q9MH"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import numpy as np\n",
        "\n",
        "def check(name: str, cond: bool):\n",
        "    if not cond:\n",
        "        raise AssertionError(f'Failed: {name}')\n",
        "    print(f'OK: {name}')\n",
        "\n",
        "rng = np.random.default_rng(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9fEf5nPQ9MH"
      },
      "source": [
        "## Section 0 — Toy dataset\n",
        "We use a tiny spam/ham dataset so you can verify logic easily."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "dXxkXkM3Q9MH"
      },
      "outputs": [],
      "source": [
        "docs = [\n",
        "    ('spam', 'win money now'),\n",
        "    ('spam', 'free money win'),\n",
        "    ('spam', 'claim your free prize'),\n",
        "    ('ham',  'meeting schedule tomorrow'),\n",
        "    ('ham',  'let us schedule a meeting'),\n",
        "    ('ham',  'project update tomorrow'),\n",
        "]\n",
        "\n",
        "labels = np.array([1 if y=='spam' else 0 for y,_ in docs])\n",
        "texts = [t for _,t in docs]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGByoReFQ9MI"
      },
      "source": [
        "## Section 1 — Tokenization + Vocabulary\n",
        "\n",
        "### Task 1.1: Tokenize\n",
        "\n",
        "# TODO: implement tokenizer\n",
        "# HINT: lowercase + split on non-letters\n",
        "\n",
        "**FAANG gotcha:** punctuation/case can break counts."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHEqq4XxQ9MI",
        "outputId": "822cc255-ff5a-4e4a-d425-b71987209489"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['free', 'money']\n"
          ]
        }
      ],
      "source": [
        "def tokenize(text):\n",
        "    # TODO\n",
        "    tokens = re.split(r'[^a-zA-Z]+', text.lower())\n",
        "    return [t for t in tokens if t]\n",
        "    # return tokens\n",
        "\n",
        "print(tokenize('Free MONEY!!!'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3TyW_tYQ9MI"
      },
      "source": [
        "### Task 1.2: Build vocabulary\n",
        "\n",
        "# TODO: build vocab dict word->index from all docs\n",
        "\n",
        "**Checkpoint:** What happens if a word appears only in test?\n",
        "\n",
        "**Answer:** If a word appears only in the test set, it’s out-of-vocabulary and ignored, since the model can’t assign learned statistics to unseen words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mzF8OxLgQ9MI",
        "outputId": "1ba15b98-7e02-4d9d-ab7f-b29f679ce87d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "inverese vocabulary :  {0: 'win', 1: 'money', 2: 'now', 3: 'free', 4: 'claim', 5: 'your', 6: 'prize', 7: 'meeting', 8: 'schedule', 9: 'tomorrow', 10: 'let', 11: 'us', 12: 'a', 13: 'project', 14: 'update'}\n",
            "{'win': 0, 'money': 1, 'now': 2, 'free': 3, 'claim': 4, 'your': 5, 'prize': 6, 'meeting': 7, 'schedule': 8, 'tomorrow': 9, 'let': 10, 'us': 11, 'a': 12, 'project': 13, 'update': 14}\n",
            "OK: vocab_nonempty\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "vocab = {}\n",
        "for t in texts:\n",
        "  for w in tokenize(t):\n",
        "    if w not in vocab:\n",
        "      vocab[w] = len(vocab)\n",
        "\n",
        "inv_vocab = {i:w for w,i in vocab.items()}\n",
        "print(\"inverese vocabulary : \", inv_vocab)\n",
        "\n",
        "print(vocab)\n",
        "check('vocab_nonempty', len(vocab) > 0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P3HiHiL2Q9MI"
      },
      "source": [
        "## Section 2 — Vectorization (Count vectors)\n",
        "\n",
        "### Task 2.1: Convert docs to count matrix X (n_docs, |V|)\n",
        "\n",
        "# TODO\n",
        "# HINT: loop over tokens inside a doc is ok; avoid loops over vocab per doc\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CEavOwLlQ9MI",
        "outputId": "72144264-188c-4d33-ac8f-460a343650fe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X shape (6, 15)\n",
            "OK: shape\n"
          ]
        }
      ],
      "source": [
        "def vectorize(texts, vocab):\n",
        "    # TODO\n",
        "    X = np.zeros((len(texts), len(vocab)), dtype=np.int64)\n",
        "    for i, text in enumerate(texts):\n",
        "      for w in tokenize(text):\n",
        "        if w in vocab:\n",
        "          X[i, vocab[w]] += 1\n",
        "    return X\n",
        "\n",
        "X = vectorize(texts, vocab)\n",
        "print('X shape', X.shape)\n",
        "check('shape', X.shape[0] == len(texts) and X.shape[1] == len(vocab))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SZGYd0njQ9MI"
      },
      "source": [
        "## Section 3 — Train Multinomial Naive Bayes\n",
        "\n",
        "### Task 3.1: Fit with Laplace smoothing\n",
        "\n",
        "Compute:\n",
        "- class priors P(y)\n",
        "- word likelihoods P(word|y) with Laplace smoothing alpha\n",
        "\n",
        "# HINT:\n",
        "- count words per class: sum rows where y==c\n",
        "- add alpha to each word count\n",
        "\n",
        "**Checkpoint:** Why smoothing is required?\n",
        "\n",
        "**Answer:** Smoothing prevents zero probabilities for unseen words, which would otherwise dominate the Naive Bayes score and make the classifier unusable.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0qV4NPdOQ9MI",
        "outputId": "40e1f1d0-c120-4d6e-86a5-dc28d6e0dfe1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "OK: priors_shape\n",
            "OK: lik_shape\n"
          ]
        }
      ],
      "source": [
        "def fit_nb(X, y, alpha=1.0):\n",
        "    # TODO\n",
        "    # return log_priors (2,), log_likelihoods (2, V)\n",
        "    y = y.astype(int)\n",
        "    V = X.shape[1]\n",
        "    log_priors = np.zeros(2)\n",
        "    log_lik = np.zeros((2, V))\n",
        "    for c in (0, 1):\n",
        "      Xc = X[y == c]\n",
        "      log_priors[c] = np.log(Xc.shape[0] / X.shape[0])\n",
        "      word_counts = Xc.sum(axis=0)\n",
        "      smoothed = word_counts + alpha\n",
        "      denom = smoothed.sum()\n",
        "      log_lik[c] = np.log(smoothed / denom)\n",
        "    return log_priors, log_lik\n",
        "\n",
        "log_priors, log_lik = fit_nb(X, labels, alpha=1.0)\n",
        "check('priors_shape', log_priors.shape == (2,))\n",
        "check('lik_shape', log_lik.shape[0] == 2 and log_lik.shape[1] == X.shape[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_9XbpafoQ9MI"
      },
      "source": [
        "### Task 3.2: Predict in log-space\n",
        "\n",
        "For each doc:\n",
        "log P(y=c|x) ∝ log P(y=c) + sum_j x_j * log P(word_j|c)\n",
        "\n",
        "**Interview Angle:** Why use logs?\n",
        "\n",
        "**Answer:** We use logs for numerical stability (avoid underflow) and faster computation (sums instead of products)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RBvstuzaQ9MJ",
        "outputId": "e84b17e5-2f6d-4e66-81b9-cb0d9eb3fc01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "pred [1 1 1 0 0 0]\n",
            "train_acc 1.0\n",
            "OK: acc_reasonable\n"
          ]
        }
      ],
      "source": [
        "def predict_nb(X, log_priors, log_lik):\n",
        "    # TODO\n",
        "    scores = X @ log_lik.T + log_priors  # broadcast priors over rows\n",
        "    return np.argmax(scores, axis=1)\n",
        "\n",
        "pred = predict_nb(X, log_priors, log_lik)\n",
        "print('pred', pred)\n",
        "acc = float(np.mean(pred == labels))\n",
        "print('train_acc', acc)\n",
        "check('acc_reasonable', acc >= 0.8)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9o35PGQ5Q9MJ"
      },
      "source": [
        "## Section 4 — Error Analysis\n",
        "\n",
        "### Task 4.1: Most influential tokens per class\n",
        "Show top tokens by log-likelihood ratio between classes.\n",
        "\n",
        "# HINT: sort (log_lik[spam] - log_lik[ham])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YiN_16CIQ9MJ",
        "outputId": "33cb4d10-43cb-4455-c553-51b10cf45fc2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "top spam tokens: ['win', 'money', 'free', 'now', 'claim']\n"
          ]
        }
      ],
      "source": [
        "# TODO\n",
        "inv_vocab = {i:w for w,i in vocab.items()}\n",
        "score = log_lik[1] - log_lik[0]\n",
        "top = np.argsort(-score)[:5]\n",
        "print('top spam tokens:', [inv_vocab[i] for i in top])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RraDjSUXQ9MJ"
      },
      "source": [
        "---\n",
        "## Submission Checklist\n",
        "- All TODOs completed\n",
        "- Train accuracy shown\n",
        "- Top tokens printed\n",
        "- Checkpoint questions answered\n"
      ]
    }
  ]
}